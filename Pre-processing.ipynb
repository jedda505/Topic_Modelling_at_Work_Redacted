{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.data.path.append(\"../xxxxxxxxx/nltk_data\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "import re\n",
    "\n",
    "rand_seed = 96\n",
    "# nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Read in raw data\n",
    "\n",
    "data = pd.read_csv('s3://xxxxxx/xxxxxx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MEANING'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- filter data for only 'sr' rf/rj\n",
    "\n",
    "df = data[(data['MEANING'].str.contains('sr', case=False)) & \n",
    "           (~data['MEANING'].str.contains('ap', case=False))]\n",
    "\n",
    "df['MEANING'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['C_NAME'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['C_NAME'].str.contains('Family', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Creating a copy as a working dataframe\n",
    "\n",
    "wdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --Drop unnecesary columns\n",
    "\n",
    "wdf = wdf.loc[:, wdf.columns.intersection(['D_STATUS','MEANING','DESCRIPTION','NOTES'])]\n",
    "\n",
    "# --Drop nan values from notes\n",
    "\n",
    "wdf = wdf.dropna()\n",
    "\n",
    "wdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- defining functions for pre-processing\n",
    "\n",
    "def change_to_str(itext):\n",
    "    # -- seperate rows with spacebar\n",
    "    return \" \".join(itext)\n",
    "\n",
    "\n",
    "def lowercase(itext):\n",
    "    # -- returns rows as lower case version\n",
    "    return itext.lower()\n",
    "\n",
    "\n",
    "def remove_punc(itext):\n",
    "    # -- removes punctuation marks / replaces with nothing\n",
    "    for each_punctuation_mk in string.punctuation:\n",
    "        itext = itext.replace(each_punctuation_mk, \"\")\n",
    "    return itext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- removing numbers\n",
    "wdf['PROC_NOTES'] = wdf['NOTES'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- applying functions\n",
    "wdf['NOTES'] = wdf['NOTES'].apply(lowercase)\n",
    "\n",
    "wdf['PROC_NOTES'] = wdf['PROC_NOTES'].apply(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf['PROC_NOTES'] = wdf['PROC_NOTES'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdf['PROC_NOTES'] = wdf['PROC_NOTES'].apply(change_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- View dataframe to check all is correct\n",
    "\n",
    "wdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Create two seperate dataframes for rf and ionsions to analyse seperately\n",
    "\n",
    "rf = wdf[wdf['D_STATUS'].str.contains('rf', case = False)]\n",
    "\n",
    "rj = wdf[wdf['D_STATUS'].str.contains('rj', case = False)]\n",
    "\n",
    "print(rf['D_STATUS'].unique())\n",
    "print(rj['D_STATUS'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Create CSV exports of data\n",
    "\n",
    "rf.to_csv('s3://xxxxxxxxxxxxx/rf.csv')\n",
    "\n",
    "rj.to_csv('s3://xxxxxxxxxxxx/rj.csv')\n",
    "\n",
    "wdf.to_csv('s3://xxxxxxxxxxx/combined.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec_reasons_venv_py3",
   "language": "python",
   "name": "spec_reasons_venv_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
