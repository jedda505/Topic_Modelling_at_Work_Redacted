{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "import nltk\n",
    "nltk.data.path.append(\"../xxxxxxxx/nltk_data\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "import re\n",
    "import num2words\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "# from gensim.models import HdpModel\n",
    "# from gensim.models import Nmf\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "rand_seed = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Read in CSVs\n",
    "\n",
    "# --rj\n",
    "\n",
    "#wdf = pd.read_csv('s3://xxxxxxxxxxxxxx/rj.csv')\n",
    "\n",
    "# -- rf \n",
    "wdf = pd.read_csv('s3://xxxxxxxxxxxxxx/rf.csv')\n",
    "\n",
    "# -- all_notes\n",
    "# wdf = pd.read_csv('s3://xxxxxxxxxxxxxx/combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(itext):\n",
    "    # -- tokenize words for processing\n",
    "    return nltk.word_tokenize(itext)\n",
    "\n",
    "def senttokenizer(itext):\n",
    "    # -- tokenize sentences for processing\n",
    "    return nltk.sent_tokenize(itext)\n",
    "\n",
    "def lemmatizer(itokens):\n",
    "    # -- lemmatize words - to be used after tokenization\n",
    "    lem = WordNetLemmatizer()\n",
    "    return[lem.lemmatize(token) for token in itokens]\n",
    "\n",
    "def rem_stopwords(itokens):\n",
    "    # -- Removes stopwords including all customer stop words from processed word tokens.\n",
    "    # -- Add your own stop words to custom_stop_words using '','',''\n",
    "    custom_stop_wds = ['xxxx','yyyy','zzzz', 'aaaa', 'bbbb', 'cccc', 'dddd', 'eeee', 'ffff', 'gggg']\n",
    "    stop_words = set(stopwords.words('english') + custom_stop_wds)\n",
    "    return [token for token in itokens if token not in stop_words]\n",
    "\n",
    "def remove_num(itokens):\n",
    "    # -- Remove numbers from processed word tokens\n",
    "    return [token for token in itokens if token.isalpha()]\n",
    "\n",
    "def remove_short_tokens(itokens):\n",
    "    # -- Remove word token when token is short - less than 2 letters\n",
    "    return [token for token in itokens if len(token) > 2]\n",
    "\n",
    "def pstem(itokens):\n",
    "    # -- Return word stems (porter stemmer) for all tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in itokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- proc_notes column to str to be processed\n",
    "\n",
    "wdf['PROC_NOTES'] = wdf['PROC_NOTES'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- unwanted phrases removed from analysis... don't carry meaning on their own / in this context\n",
    "\n",
    "wdf['PROC_NOTES'] = wdf['PROC_NOTES'].str.replace('xxxxxxxxxxx', '')\n",
    "wdf['PROC_NOTES'] = wdf['PROC_NOTES'].str.replace('yyyyyyyyyyyyy', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --tokenize\n",
    "\n",
    "wdf['PROC_TOKENS'] = wdf['PROC_NOTES'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- remove stop words\n",
    "\n",
    "wdf['PROC_TOKENS'] = wdf['PROC_TOKENS'].apply(rem_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- remove the numbers\n",
    "\n",
    "wdf['PROC_TOKENS'] = wdf['PROC_TOKENS'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- remove the short words, less than 2 letters\n",
    "\n",
    "wdf['PROC_TOKENS'] = wdf['PROC_TOKENS'].apply(remove_short_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- lemmatize\n",
    "\n",
    "wdf['TOKENS_LEMM'] = wdf['PROC_TOKENS'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- stemming\n",
    "wdf['TOKENS_STEMS'] = wdf['PROC_TOKENS'].apply(pstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Preparation for Gensim models\n",
    "\n",
    "gensim_tokens = wdf[\"TOKENS_LEMM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict = corpora.Dictionary(gensim_tokens)\n",
    "\n",
    "print(gensim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_corpus = [gensim_dict.doc2bow(token) for token in gensim_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch for params (N/A for Gridsearch models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Run NMF\n",
    "\n",
    "# -- GridSearchCV reccomended parameters (SKLearn) = {'alpha_H': 0, 'alpha_W': 0.1, 'l1_ratio': 0.1, 'n_components': 8, 'random_state': 1}\n",
    "\n",
    "NMF = gensim.models.nmf.Nmf(corpus = gensim_corpus, id2word = gensim_dict, num_topics = 14, random_state=rand_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Run LDA\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel(corpus = gensim_corpus, id2word= gensim_dict, num_topics=22, random_state=rand_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Run HDP Model\n",
    "\n",
    "HDP = gensim.models.hdpmodel.HdpModel(corpus = gensim_corpus, id2word=gensim_dict, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF.top_topics(corpus=gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.top_topics(corpus=gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDP.print_topics(num_topics=15, num_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- LDA Vis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vislda = pyLDAvis.gensim_models.prepare(LDA, gensim_corpus, gensim_dict)\n",
    "vislda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Enter search for words / phrases and select a sample\n",
    "\n",
    "words = ['','']\n",
    "\n",
    "regex_words = []\n",
    "\n",
    "for word in words:\n",
    "    regex_words.append(\"(?=.*\" + word + \")\")    \n",
    "\n",
    "regex_words\n",
    "\n",
    "# -- Make search terms upper case to highlight them\n",
    "\n",
    "searchdf = pd.DataFrame(wdf['NOTES'][wdf['NOTES'].str.contains(''.join(regex_words), case=False)])\n",
    "\n",
    "sample = searchdf.sample(n=10)\n",
    "\n",
    "for i in range(0,len(sample['NOTES'])):\n",
    "    for j in words:\n",
    "        sample.iat[i,0] = sample.iat[i,0].replace(j, str(j).upper())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(searchdf),\"/\", len(wdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec_reasons_venv_py3",
   "language": "python",
   "name": "spec_reasons_venv_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
